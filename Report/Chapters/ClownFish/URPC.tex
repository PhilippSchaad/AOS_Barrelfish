\section{URPC}\label{s:URPC}

As we do not have a fully unified system, we have the init<->init cross core 
communication system which is dubbed URPC.
\medskip

Originally it was just a single page split into two 2 roughly 2kb section 
(slightly less as we had a smaller shared head at the top of the page) and 
could just transfer messages in ~2kb blogs and signalling the other side that 
the message arrived.
\medskip

This got adapted when the actual cross core communication chapter came half a week later and required a handling using cache lines. So we 
adapted it to be chunked and used that opportunity to adopt it to a similar 
framework system as the RPC one (see \ref{ss:rpc_implementation} for more details).
\medskip

As the URPC system does not involve the kernel, it turned out to be a much 
simpler system and way less error prone. We segmented the page into 64byte chunks. 1 byte is used 
for flags and 63 bytes are used for data. We kept track of these chunks using a ringbuffer. We 
learned from the RPC system here and did a better encoding. Because urpc only has to deal with a single "channel" and therefore a single FIFO queue of messages, we only send the length and the metadata once per logical message. 
The flag byte is primarily used to signify if this entry is empty or written 
to, as well as what encoding we will be using. 
\medskip

A good side-effect of our implementation is, that because we send and receive in a loop, we only need a single 
memory barrier in each loop. 
\medskip

On top of these two core loops, we run an endless loop which polls for available messages or messages ready to be sent.
\medskip

On top of that is a somewhat 
familiar machinery. We can enqueue messages and register receive handlers. 
\medskip

For implementing the actual URPC calls we first considered making an RPC 
equivalent for every single URPC call, but then instead added some more 
logic which allowed us to root an arbitrary RPC call to init over 
URPC, thus allowing the two init processes to act as if they were a single one 
for the purposes of RPC.
An init process receives a fake call to its rpc receive handler with a 
normal recv\_list struct pointer, except that the channel points to NULL, which can 
never happen in a normal RPC call and is thus a fairly obvious and simple 
distinguisher between an RPC or URPC call.
\medskip

In theory, this same system could be used to allow an arbitrary pair of 
processes to communicate directly with each other, if they can be arranged to 
share memory. This, however, we never got around to implementing and instead 
just implemented a proxy service using the init processes for communication. 
This has the benefit of not having the extra looping thread overhead and is 
easier to set up than sharing physical memory across cores (since that would 
require us to mint and track a new ram cap. Our current memory manager and the
incomplete implementation of cap\_revoke would render this difficult.)
