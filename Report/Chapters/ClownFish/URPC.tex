\section{URPC}\label{s:URPC}

As we do not have a fully unified system we have the init<->init cross core 
communication bit which is dubbed URPC.
\medskip

Originally it was just a single page split into two 2 roughly 2kb section 
(slightly less as we had a smaller shared head at the top of the page) and 
could just transfer messages in ~2kb blogs and signalling the other side that 
the message arrived.
\medskip

This got adapted when the actual cross core communication chapter came up and 
said it should be done in cache lines. So we 
adapted it to be chunked and used that opportunity to adopt it to a similiar 
framework system as the RPC one.
\medskip

As the URPC system does not involve the kernel it turned out to be a much 
simpler system and way less error prone. We segmented the page into 64byte chunks, which had 1 byte 
for flags and 63 bytes for data and we kept track of them as a ringbuffer. We 
learned from the RPC system here and did a better encoding. Because urpc only has to deal with a single "channel" and therefore a single FIFO queue of messages, we only send the length and the metadata once. 
The flag byte is primarily used to signify if this entry is empty or written 
to, as well as what encoding we will be using. 
\medskip

A good side-effect of our implementation is, that because we send and receive in a loop, we only need a single 
memory barrier each. 
\medskip

On top of these two core loops, we run an endless loop which polls for available messages or messages ready to be sent.
\medskip

On top of that is a somewhat 
familiar machinary. We can enqueue messages and register receive handlers. 
\medskip

For implementing the actual URPC calls we first considered making an RPC 
equivalent for every single URPC call, but then instead added some more 
machinary which allowed us to root an arbitrary RPC call to init over 
URPC, thus allowing the two init processes to act as if they were a single one 
for the purposes of RPC.
\medskip

An init process just receives a fake call to its rpc receive handler with a 
normal recv\_list struct, except that the channel is set to NULL, which can 
never happen in a normal RPC call and is thus a fairly obvious and simple 
distinguisher between an RPC or URPC call.
\medskip

In theory, this same system could be used to allow an arbitrary pair of 
processes to communicate directly with each other, if they can be arranged to 
share memory. This, however, we never got around to implementing and instead 
just implemented a proxy service using the init processes for communication. 
This has the benefit of not having the extra looping thread overhead and is 
easier to set up than sharing physical memory across cores (since that would 
require us to mint and track a new ram cap. Our current memory manager and the
incomplete implementation of cap\_revoke would render this difficult.)
