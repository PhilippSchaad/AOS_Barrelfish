\section{RPC}\label{s:RPC}

For terminology, our actual remote procedure call system is split into two 
sections:
The RPC system, which is build on lmp\_chan's and for transferring messages 
between processes on the same core, and the URPC system, which is build on a 
shared memory page and is for transferring messages between the init processes 
on the two cores.
\medskip

Now, our biggest regret is that we never unified those two systems properly. 
An approach at unification which would have involved rewriting aos\_rpc to 
account for that could have likely provided an uniform interface and saved us 
a lot of pain along the way. But we didn't get there.

\subsection{Early struggles}
The development of the RPC system started out as an adhoc implementation of 
just adding one RPC call after another and having all the logic in every single
handler. This was, naturally, horribly error prone and hard to maintain or upgrade. 
Also, it was based on a misunderstanding on how to approach this. We 
registered a send and recv handler on the channel every time we made one of 
those RPC calls, this turned out to lead to massive issues with multithreading 
and meant processes could only receive messages if they had just sent one. 
We then had 
a single shared recv handler, but the memory RPC call needed its own recv 
handler as that design didn't quite work for that one. "The memory RPC call 
needs to be handled extra" is a common theme that would repeat throughout our 
changes to this system.
\medskip

At first we tried to make some macros and functions while keeping the same 
basic structure, but that too turned out to be unmanageable. The time for a 
rewrite of the first version of the current system had come. 

\subsection{Rewrite}
Instead of init and other processes both doing their own ad-hoc things with 
the RPC setup and calls, we developed a single shared framework.
This framework is set up in such a way that first we automatically grab a fresh ID to 
mark the current logical call. This way we can associate responses with it. After that, the memory for the logical call gets persisted and enqueued in the list of 
messages to send. Further, we register the message on the channel for sending if 
nothing is registered there yet.  
When it is time to send, the message automatically gets a standardised encoding 
scheme applied to it: the first 32bit word of every physical message gets 
encoded to contain the type of call, id of the call and length of the total 
payload of the logical message this is part of. Then, the actual sending takes 
place until all parts of the message are sent or an issue occured.\\

\medskip
Further, the framework also has a standardised receive handler, which receives 
all calls on that channel and recovers the logical message from the physical 
messages sent. It does so based on the type, id and a list of  
unfinished calls to match the type and id to. 
When a message is fully reassembled, we call a process/channel specific handler. Here, an optimisation opportunity for small 
messages (which are the vast majority) presented itself. Instead of 
persisting it all and adding it to the list and doing all the other steps involved, we can do it malloc-free by passing that element to the handler on the stack. This works because we return into this function again and in the normal case free the memory.\\

\medskip
While this framework now standardised the encoding of messages and made it 
really convenient to work with rpc calls, it also presented a large increase in complexity of the code in one 
particular place. However, it reduced the overall complexity by reducing the complexity and gain managability in many other places.
The rewrite, however, still had issues. 

\subsection{RAM rpc calls}
After the first series of minor bugs 
was fixed, we ran into a pretty big issue: The ram RPC handler now ran over 
that system, which used mallocs in various places due to its generality and 
the thus arising need to persist various sizes etc.
Our first and naive approach was to add special 
conditions for ram\_rpc calls. We relied on using buffers of the 
structs involved instead of allocating new memory and therefore taking a slightly special path. 
This, while working for some time, was naturally doomed to fail as we still got 
pagefaults at times and the pile of patches that held our os together was 
ever increasing.\\
\medskip
The next step then was to move the ram\_rpc procedure partially out of the system. 
Receiving calls is still part of it, sending is not. This cut down on the amount of memory allocations 
further and we had a buffer for the rest, which got us a bit 
further. But, the actual solution to this problem came later, after it 
had plagued us for weeks with occurring only spuriously and the code around it 
being annoyingly complex and hard to comprehend.
The solution was, that the ram RPC got moved into its own channel, 
independently of the others. Up to this point, all domains created only one single
channel to init. We gave the ram rpc call its own machinery, very reminiscent of the original 
implementation. Luckily, on the init side, it did not have to be handled 
differently.
However, we still got pagefaults and the occasional strange errors, which 
later got tracked down to a critical bug in the sending routine of the RPC system. It used 
only a single queue for sending instead of a per-channel queue, which meant 
that in particular cases it was possible for messages not to get sent to the 
proper channel. In other cases, a message was tried to send despite the wrong channel being ready. This got fixed after a lot of painful investigation as it 
was hard to track down that this caused the errors.

\subsection{Implementation}
The current RPC subsystem system is split in two parts. In 
aos\_rpc\_shared is (most of) the machinery described so far.
\medskip

\begin{lstlisting}[caption={RPC init function prototypes}, 
label=lst:rpc_calls_1, numbers=left, stepnumber=1, float, floatplacement=tl, 
frame=tb, language=c]
errval_t init_rpc_server(
             void (*recv_deal_with_msg)(struct recv_list *),
             struct lmp_chan *chan);

errval_t init_rpc_client(
             void (*recv_deal_with_msg)(struct recv_list *),
             struct lmp_chan *chan, 
             struct capref dest);
\end{lstlisting}
\begin{lstlisting}[caption={RPC send function prototype}, 
label=lst:rpc_calls_2, numbers=left, stepnumber=1, float, floatplacement=tl, 
frame=tb, language=c]
errval_t send(struct lmp_chan *chan, struct capref cap, 
              unsigned char type, size_t payloadsize, 
              uintptr_t *payload, 
              struct event_closure callback_when_done, 
              unsigned char id);
\end{lstlisting}

The most important functions are shown in \autoref{lst:rpc_calls_1}.
init\_rpc\_client creates a new lmp\_chan. It 
creates it with an empty remote\_cap, waiting for anyone to connect to it. 
Also, it uses the handler provided as recv\_deal\_with\_msg as the callback to 
use when a logical message is fully reconstructed on this channel. The struct 
recv\_list contains the payload (both the data and the eventual capref) 
and its metadata (rpc call type, id, size).
This is logically the same as init\_rpc\_server, except there you need to provide 
 a remote cap to connect to.
\medskip

\autoref{lst:rpc_calls_2} depicts the primary way to send messages, it does 
all the explained converting, enqueuing and registering (send\_loop does the 
actual partitioning and sending).
\medskip

Something that at the time seemed useful was having a callback to be called 
once the message was fully sent, but in practice our system never evolved to 
be callback based enough for this to become useful. 
A feature missing, is a callback to be called in case sending fails, so 
that we can do custom and message specific handling of issues.
\medskip

\begin{lstlisting}[caption={RPC send\_response prototype}, 
label=lst:rpc_calls_3, numbers=left, stepnumber=1, float, floatplacement=tl, 
frame=tb, language=c]
errval_t send_response(struct recv_list *rl, 
                       struct lmp_chan *chan,
                       struct capref cap, 
                       size_t payloadsize, 
                       void *payload);
\end{lstlisting}
\begin{lstlisting}[caption={rpc\_framework prototype}, 
label=lst:rpc_calls_4, numbers=left, stepnumber=1, float, floatplacement=tl, 
frame=tb, language=c]
rpc_framework(void (*inst_recv_handling)(void *arg1, 
                  struct recv_list *data),
              void *recv_handling_arg1,
              unsigned char type,
              struct lmp_chan *chan, 
              struct capref cap, 
              size_t payloadsize, 
              uintptr_t *payload,
              struct event_closure 
                  callback_when_done)
\end{lstlisting}

Finally, \autoref{lst:rpc_calls_3} shows the convenience function 
send\_response, which answers a RPC call with the given cap and data. 
The real gain, however, was not in having those functions, but that they 
allowed us to write the rpc\_framework function (shown in 
\autoref{lst:rpc_calls_4})), which looks really wild - and it is. 
It works in conjunction with a logical message recv handler 
which is set up in such a way that it takes the type and id info we were given 
and looks it up in the list of calls that we made and which we are await an 
answer from.
\medskip

rpc\_framework takes 
the function which handles the response to the call are about to make, as 
well as some state for it (arg1 will be passed to the handler). 
The other parameters are used to provide the info needed for making the call.
Additionally, it registers for a response. After that, it sends out the message and 
pumps the waitset until it receives an answer (thus being blocking). 
This means all we now needed to do to add another RPC call is to implement a 
very simple recv handler and 
serialise our data for sending.
\medskip

\begin{lstlisting}[caption={aos\_rpc\_get with helper function}, 
label=lst:rpc_example, numbers=left, stepnumber=1, float, floatplacement=tl, 
frame=tb, language=c]
static void get_nameserver_recv_handler(
    void *arg1, struct recv_list *data)
{
    struct capref *retcap = (struct capref *) arg1;
    *retcap = data->cap;
}

errval_t aos_rpc_get_nameserver(
    struct aos_rpc *rpc, struct capref *retcap)
{
    rpc_framework(get_nameserver_recv_handler,
                  (void *) retcap,
                  RPC_TYPE_GET_NAME_SERVER, 
                  &rpc->chan, NULL_CAP,
                  0, NULL, NULL_EVENT_CLOSURE);
    return SYS_ERR_OK;
}
\end{lstlisting}

\autoref{lst:rpc_example} shows an example for a call for getting the 
nameserver's cap (so we can connect with all the other services by asking the 
nameserver for those).
\medskip

Overall, the RPC system had a long history and features a great API now. One that is  simple to use 
and does not need a deep understanding of how lmp\_chan's work. However, this 
was paid for in bugs, complexity and a lot of time. However, in the end, it was a worthwhile 
tradeoff as this setup can be extend rapidly.
\medskip

However, our one big drawback is that we never made it backend agnostic to integrate 
the cross-core communication system (dubbed URPC) into it.
